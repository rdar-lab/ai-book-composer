# AI Book Composer Configuration

# LLM Configuration
llm:
  provider: ollama_embedded  # Options: openai, gemini, azure, ollama, ollama_embedded
  model: llama-3.2-3b-instruct
  temperature:
    planning: 0.3
    execution: 0.7
    critique: 0.2

# Provider-specific settings
providers:
  openai:
    api_key: ${OPENAI_API_KEY}  # Can use environment variables
  
  gemini:
    api_key: ${GOOGLE_API_KEY}
  
  azure:
    api_key: ${AZURE_OPENAI_API_KEY}
    endpoint: ${AZURE_OPENAI_ENDPOINT}
    deployment: ${AZURE_OPENAI_DEPLOYMENT}
  
  ollama:
    base_url: http://localhost:11434
    model: llama2  # Specify the model name for Ollama
  
  ollama_embedded:
    # Embedded (in-process) ollama execution using llama.cpp
    model_name: llama-3.2-3b-instruct  # Model name to download from HuggingFace
    n_ctx: 2048  # Context window size
    n_threads: 4  # Number of CPU threads to use
    run_on_gpu: false  # Use GPU acceleration if available
    verbose: false  # Enable verbose output

# Whisper Configuration (for audio/video transcription)
whisper:
  mode: local  # Options: local, remote
  model_size: base  # Options: tiny, base, small, medium, large
  # For remote mode (containerized):
  remote:
    endpoint: http://localhost:9000
    api_key: null
  # For local mode:
  local:
    device: cpu  # Options: cpu, cuda
    compute_type: int8

# Text File Reading Configuration
text_reading:
  max_lines_per_read: 100  # Maximum lines to read at a time from text files
  supported_formats:
    - txt
    - md
    - rst
    - docx
    - rtf
    - pdf

# Image Processing Configuration
image_processing:
  supported_formats:
    - jpg
    - jpeg
    - png
    - gif
    - bmp
  extract_from_pdf: true  # Extract images from PDF files
  max_image_size_mb: 10  # Maximum size per image
  max_images_per_chapter: 5  # Maximum images to place per chapter

# Audio/Video Processing Configuration
media_processing:
  audio_formats:
    - mp3
    - wav
    - m4a
    - flac
    - ogg
  video_formats:
    - mp4
    - avi
    - mov
    - mkv
  # Chunk size for large files (in seconds)
  chunk_duration: 300  # 5 minutes per chunk
  max_file_duration: 3600  # Max 1 hour per file

# Book Generation Configuration
book:
  output_language: en-US
  default_title: Composed Book
  default_author: AI Book Composer
  quality_threshold: 0.7
  max_iterations: 3

# Parallel Execution Configuration
parallel:
  parallel_execution: true  # true = enabled, false = disabled
  parallel_workers: 4  # Number of worker threads/processes for parallel execution

# Logging Configuration
logging:
  level: INFO  # Options: DEBUG, INFO, WARNING, ERROR
  file: logs/ai_book_composer.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console_output: false  # Don't print to console, use file only

# Security Configuration
security:
  allow_directory_traversal: false
  max_file_size_mb: 500

# MCP Server Configuration
mcp_server:
  host: 127.0.0.1
  port: 8000
  name: ai-book-composer
  # Enable debug mode for development
  debug: false
  # Log level for MCP server
  log_level: INFO

