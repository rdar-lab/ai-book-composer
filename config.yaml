# AI Book Composer Configuration

# LLM Configuration
llm:
  provider: ollama_embedded  # Options: openai, gemini, azure, anthropic, bedrock, ollama, ollama_embedded
  model: llama-3.2-3b-instruct
  temperature:
    planning: 0.3
    execution: 0.7
    critique: 0.2

# Provider-specific settings
providers:
  openai:
    api_key: ${OPENAI_API_KEY}  # Can use environment variables
  
  gemini:
    api_key: ${GOOGLE_API_KEY}
  
  azure:
    api_key: ${AZURE_OPENAI_API_KEY}
    endpoint: ${AZURE_OPENAI_ENDPOINT}
    deployment: ${AZURE_OPENAI_DEPLOYMENT}
  
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}  # Get your API key from https://console.anthropic.com/
  
  bedrock:
    region_name: ${AWS_REGION}  # e.g., us-east-1, us-west-2
    aws_access_key_id: ${AWS_ACCESS_KEY_ID}  # Optional if using AWS credentials profile
    aws_secret_access_key: ${AWS_SECRET_ACCESS_KEY}  # Optional if using AWS credentials profile
    aws_session_token: ${AWS_SESSION_TOKEN}  # Optional, for temporary credentials
    profile_name: null  # Optional: AWS credentials profile name (e.g., 'default')
  
  ollama:
    base_url: http://localhost:11434

  ollama_embedded:
    # Embedded (in-process) ollama execution using llama.cpp
    internal:
      n_ctx: 2048  # Context window size
      n_threads: 4  # Number of CPU threads to use
      n_batch: 64
      verbose: false  # Enable verbose output
    run_on_gpu: false  # Use GPU acceleration if available

# Whisper Configuration (for audio/video transcription)
whisper:
  mode: local  # Options: local, remote
  model_size: base  # Options: tiny, base, small, medium, large
  # For remote mode (containerized):
  remote:
    endpoint: http://localhost:9000
    api_key: null
  # For local mode:
  local:
    device: cpu  # Options: cpu, cuda
    compute_type: int8

# Image Processing Configuration
image_processing:
  supported_formats:
    - jpg
    - jpeg
    - png
    - gif
    - bmp
  extract_from_pdf: true  # Extract images from PDF files
  max_image_size_mb: 10  # Maximum size per image
  max_images_per_chapter: 5  # Maximum images to place per chapter

# Vision Model Configuration (for image description)
vision_model:
  provider: openai  # Options: openai, gemini, azure, ollama
  model: gpt-4o-mini  # Vision-capable model (gpt-4o, gpt-4o-mini, gemini-1.5-flash, etc.)
  temperature: 0.3

# Book Generation Configuration
book:
  output_language: en-US
  default_title: Composed Book
  default_author: AI Book Composer
  quality_threshold: 0.7
  max_iterations: 3
  style_instructions: ""  # Optional: Guide the AI on book style (e.g., "I want an academic book", "I want it to be light reading", "I want it to be professional reading material")

# Parallel Execution Configuration
parallel:
  parallel_execution: true  # true = enabled, false = disabled
  parallel_workers: 4  # Number of worker threads/processes for parallel execution

# RAG (Retrieval Augmented Generation) Configuration
rag:
  enabled: true  # Enable RAG-based document retrieval
  embedding_model: all-MiniLM-L6-v2  # Sentence transformer model for embeddings
  chunk_size: 1000  # Size of text chunks for vector database
  chunk_overlap: 200  # Overlap between chunks

# Logging Configuration
logging:
  level: INFO  # Options: DEBUG, INFO, WARNING, ERROR
  file: logs/ai_book_composer.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console_output: false  # Don't print to console, use file only

# Security Configuration
security:
  allow_directory_traversal: false
  max_file_size_mb: 500

